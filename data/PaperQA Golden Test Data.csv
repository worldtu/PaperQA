Domain,Question No.,Question,Wrong Options,Answer,Reference Paper(s),Related Chunk(s),Background Context,
Sample,Sample,What are the key innovations in transformer architecture?,"[“Convolutional Layers”, “Recurrent Connections”, “Max Pooling”]","[“Self-Attention Mechanism”, “Multi-Head Attention”, “Parallel Processing”]","Vaswani et al., Attention Is All You Need (NeurIPS 2017)","“The Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.” ",Deep-learning architectures shifted away from recurrence and convolution to better model sequence dependencies.," ** Prompt for Background Context:
It should not be related with ""answering the question"" nor related with the papers"
CV,1,What key contribution does the Video-MME benchmark introduce for evaluating multimodal LLMs?,"[“A dataset of static images only”, “An audio-caption matching task set”, “A text-only question set for LLMs”]","[“A comprehensive video benchmark (900 videos, 254 hours) for evaluating multimodal LLMs on video understanding tasks”]","Chaoyou Fu et al., Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis (arXiv:2405.21075)","“Introduces Video-MME … a benchmark dataset (900 videos, 254 hours) for evaluating multi-modal LLMs on video understanding.”",Benchmark datasets help measure how well AI models understand information across different modalities like vision and language.,
,2,What makes the TRELLIS model distinct in 3D generation research?,"[“Uses 2D CNNs only”, “Employs GAN discriminators for mesh classification”, “Relies solely on depth maps”]","[“Introduces Structured Latent ( SLAT ) grids for encoding 3D geometry and appearance, enabling decoding into multiple 3D formats”]","Jianfeng Xiang et al., Structured 3D Latents for Scalable and Versatile 3D Generation (TRELLIS) (arXiv:2412.01506)","“SLAT encodes geometry and appearance on a sparse 3D grid, enabling decoding into radiance fields, point clouds, and meshes.”",3D generative models are increasingly used for creating digital objects in virtual and augmented reality environments.,
,3,What technique was found to most improve multimodal large language models’ performance on spatial reasoning tasks?,"[“Chain-of-thought prompting”, “Higher resolution frames only”, “Removing video input entirely”]",[“Forcing models to generate explicit spatial maps during reasoning to improve spatial memory and accuracy”],"Jihan Yang et al., Thinking in Space: How Multimodal LLMs See, Remember, and Recall Spaces (arXiv:2412.14171)",“Adding spatial map generation significantly improves model performance on spatial reasoning benchmarks.”,Understanding spatial relationships is essential for navigation tasks in robotics and video scene interpretation.,
,4,How can a single model achieve both image understanding and generation effectively?,"[“By using a single shared encoder for all tasks”, “By training two completely separate models”, “By replacing visual inputs with text only”]",[“By decoupling two visual encoders within a unified Transformer — one for understanding and one for generation — to resolve granularity conflicts”],"Chengyue Wu et al., Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation (arXiv:2410.13848)",“Janus uses two separate visual encoders inside a shared Transformer to support both analysis and synthesis tasks.”,Decoupling components in neural architectures can help manage conflicting requirements between different tasks.,
,5,What is the main innovation of the OmniGen framework for image generation?,"[“Limiting generation to text-only prompts”, “Combining GAN and diffusion in a dual pipeline”, “Using hand-crafted task adapters”]","[“Unifies multiple image generation tasks — text-to-image, editing, subject variation — in one diffusion-based model controlled by natural language instructions”]","Shitao Xiao et al., OmniGen: Unified Image Generation (arXiv:2409.11340)","“OmniGen handles text prompts, editing, and other conditional generation in a single architecture via language instructions.”",Text-to-image models are becoming integrated with interactive design tools for artistic and commercial applications.,
,6,What design feature enables a vision backbone to capture both local and global spatial dependencies efficiently?,"[“Pure convolutional residual blocks”, “Only linear layers without any recurrence”, “Frame-wise recurrent token prediction”]",[“Combining state-space Mamba layers with Transformer attention blocks to model long-range context”],"Ali Hatamizadeh & Jan Kautz, MambaVision: A Hybrid Mamba-Transformer Vision Backbone (arXiv:2407.08083)","“By combining the Mamba state-space model with Transformer layers, the architecture captures long-range spatial dependencies efficiently.”",Hybrid network backbones are widely used to balance speed and representational power in modern computer-vision models.,
,7,How does StreamingT2V maintain temporal consistency when generating long videos from text?,"[“By restarting generation for each scene chunk”, “By randomly sampling frames from the prompt”, “By limiting videos to under 10 seconds”]",[“Through short-term and long-term memory modules that preserve continuity and appearance across video segments”],"Roberto Henschel et al., StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text (arXiv:2403.14773)",“Introduces short-term memory for smooth chunk transitions and long-term memory for appearance preservation throughout videos.”,Maintaining scene continuity is a key challenge in generative video models due to motion drift and visual instability over time.,
,8,What problem does a diffusion-based video-to-depth approach solve compared to standard depth-from-video methods?,"[“Requires multi-camera setups for training”, “Generates depth maps only for synthetic data”, “Needs known camera poses and optical flow inputs”]",[“Generates temporally consistent depth maps for long videos without using camera poses or optical flow supervision”],"Wenbo Hu et al., DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos (arXiv:2409.02095)",“Produces coherent depth sequences without requiring camera poses or flow estimates by training on diffusion-based video-to-depth conversion.”,Estimating accurate scene depth from a single moving camera is a long-standing problem in structure-from-motion and AR.,
,9,What is the central idea behind the VideoTree framework for LLM video reasoning?,"[“Flatten all video frames into one embedding”, “Encode only first and last frames for efficiency”, “Use separate LLMs for each clip segment”]",[“Construct a hierarchical tree of key frames that selectively refines important video segments for efficient reasoning”],"Ziyang Wang et al., VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos (arXiv:2405.19209)",“Builds a hierarchical key-frame tree to capture multi-granular context and reduce redundant frames for video QA.”,Long videos often contain repetitive content; hierarchical summarization allows language models to reason more efficiently.,
,10,What conclusion did MambaOut draw about the usefulness of the Mamba state-space component in vision tasks?,"[“It is crucial for ImageNet classification”, “It improves training stability on all tasks”, “It enhances fine-grained segmentation accuracy universally”]","[“The state-space component adds little benefit for image classification, and removing it ( MambaOut ) improves accuracy and efficiency”]","Weihao Yu & Xinchao Wang, MambaOut: Do We Really Need Mamba for Vision? (arXiv:2405.07992)","“Removing the SSM block ( MambaOut ) outperformed Mamba-based models on ImageNet, suggesting limited benefit for short sequence tasks.”",Architectural ablation helps identify which model components genuinely contribute to performance rather than add complexity.,
,11,How can a multimodal model resolve the conflict between visual-understanding and visual-generation objectives?,"[“It uses a single encoder optimized for both tasks simultaneously”, “It discards generation and focuses only on understanding”, “It uses two separate Transformer backbones for each task”]",[“It decouples visual encoding into separate pathways for understanding and generation while using a unified transformer architecture.”],"Wu et al., Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation (arXiv:2410.13848)","“To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing.”",Unified multimodal models must reconcile different representation requirements for vision generation vs. understanding.,
,12,What is the central strategy behind the NVILA family to achieve both high accuracy and efficiency in vision-language tasks?,"[“Using fewer parameters but more data only”, “Removing vision encoder entirely and relying only on language tokens”, “Using handcrafted vision features instead of learned ones”]","[“A ‘scale-then-compress’ approach: first scale spatial/temporal resolution, then compress visual tokens to improve efficiency while preserving accuracy.”]","Liu et al., NVILA: Efficient Frontier Visual Language Models (arXiv:2412.04468)","“We improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens.”","Efficiency in large vision-language models is critical for training cost, inference latency, and deployment at scale.",
,13,"What representation does the TRELLIS model introduce, and what benefit does it provide?","[“Voxel grid with fixed resolution”, “Point cloud with no appearance encoding”, “Depth-map only representation”]","[“Structured Latent (SLAT) grids encoding geometry and appearance, enabling decoding into radiance fields, point clouds, or meshes.”]","Xiang et al., Structured 3D Latents for Scalable and Versatile 3D Generation (TRELLIS) (arXiv:2412.01506)","“SLAT encodes geometry and appearance on a sparse 3D grid, enabling decoding into different formats (radiance fields, point clouds, meshes).”",3D generation faces trade-offs between flexibility (various output formats) and efficiency.,
,14,"In WonderWorld, what enables near real-time 3D scene generation from a single image?","[“Multi-view stereo input required”, “Full mesh optimization per scene”, “Manual user editing of geometry”]",[“The FLAGS (Fast Layered Gaussian Surfels) representation enables rapid 3D scene generation from a single image.”],"Yu et al., WonderWorld: Interactive 3D Scene Generation from a Single Image (arXiv:2406.09394)","“The key innovation is FLAGS (Fast Layered Gaussian Surfels), a new 3D scene representation that can be generated very quickly from one input view.”",Single-image 3D scene creation is important for VR/AR content generation.,
,15,What is the key idea behind MoSca for reconstructing moving scenes?,"[“Static scene assumption only”, “Pre-scanned object templates”, “No camera pose estimation required at all”]","[“Lifts casual video into a 4D motion scaffold and uses Gaussian splatting to fuse geometry, appearance and motion for dynamic scene reconstruction.”]","Lei et al., MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds (arXiv:2405.17421)","“MoSca anchors Gaussian primitives on a Motion Scaffold and fuses them via Gaussian Splatting, disentangling geometry/appearance from motion.”",Capturing dynamic scenes (moving objects + camera) is a major challenge for novel-view synthesis.,
,16,How does the CUT3R framework build scene models over time?,"[“Processes each frame independently then averages results”, “Assumes static environment only”, “Outputs only 2D semantics per frame”]",[“Maintains a persistent 3D state updated with each new frame and outputs a dense point-map in a common coordinate system.”],"Wang et al., Continuous 3D Perception Model with Persistent State (CUT3R) (arXiv:2501.12387)","“The model maintains a persistent 3D state updated with each incoming image frame, incrementally building a coherent, dense reconstruction of the scene.”",Real-time or streaming 3D perception is essential for AR and robot navigation.,
,17,What enables Video-XL to handle hour-long videos efficiently?,"[“Ignore most frames and sample one per minute”, “Treat video as a single flattened tensor”, “Use only the first 5 minutes of video”]",[“Introduces Visual Summarization Tokens (VSTs) that compress chunks of frames into key-value embeddings for efficient processing.”],"Shu et al., Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding (arXiv:2409.14485)","“VSTs are special learned tokens that summarize chunks of frames into compact key-value representations, effectively performing a ‘lossy compression’ of long video input.”",Long-form video understanding requires compression and hierarchical summaries to scale.,
,18,What is the main contribution of Molmo and PixMo to open vision-language model research?,"[“They use only synthetic data for training”, “They are closed models without public weights”, “They rely entirely on proprietary model outputs for supervision”]","[“Release open-weight VLMs trained on PixMo, a fully open dataset of caption, Q&A, and pointing data collected without closed-model outputs, achieving state-of-the-art open-source performance.”]","Deitke et al., Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models (arXiv:2409.17146)",“Molmo’s largest 72B-parameter model outperforms other open VLMs and even beats some proprietary models … all weights and datasets are released publicly.”,Open data and weights promote reproducibility and democratize multimodal AI research.,
,19,What makes FlowEdit distinct among text-based image-editing methods?,"[“It requires per-image optimization”, “It depends on GAN-based discriminators”, “It uses image inversion into noise space”]",[“It is an inversion-free method that uses a flow-model-based ODE to directly map source image distributions to target text distributions for editing.”],"Kulikov et al., FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models (arXiv:2412.08629)",“FlowEdit formulates an ODE that directly transforms the source image distribution into the target text distribution without image inversion or per-image optimization.”,Efficient editing methods reduce inference time and enable wider interactive applications in image manipulation.,
,20,"What mechanism allows StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text to extend generated videos to minute-scale while preserving consistency?","[“Generates each frame independently”, “Resets appearance at each scene transition”, “Limits videos to fixed short length”]","[“Employs short-term memory for chunk transitions and long-term memory for appearance preservation, enabling smooth extension to arbitrary length”]","Roberto Henschel et al., StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text (arXiv)",“Introducing a short-term memory module for smooth chunk transitions and a long-term memory module for appearance preservation throughout videos.”,"Maintaining temporal coherence is a core challenge in long-video generation, especially from text.",
GenAI,21,"According to the study, how does GenAI affect new venture launches in categories with relatively high GenAI usage?","[""No change in launch counts"", ""Launches decreased significantly"", ""Only time-to-launch increased but not counts""]","[""The number of new venture launches increased and median time to launch decreased""]","Ruiqing Cao et al., How Founder Expertise Shapes the Impact of Generative Artificial Intelligence on Digital Ventures (arXiv:2511.06545)",“we find that the number of new venture launches increased and the median time to launch decreased significantly more in categories with relatively high GenAI usage.”,Entrepreneurs often adopt emerging technologies earlier when barriers to experimentation and prototyping are reduced.,
,22,What trend do the authors report about the “citation dividend” of review papers in the GenAI era?,"[""It is increasing across all fields"", ""Unchanged since 2010"", ""Only increases in computer science""]","[""It is declining""]","Barry Smyth et al., Have we reached the beginning of the end for review papers? (arXiv:2511.07490)","“We quantify the citation dividend associated with review papers, but also demonstrate that it is declining…”",Citation behaviors in academia can shift rapidly when automated summarization tools become widespread.,
,23,What is the paper’s central trade-off regarding data governance and GenAI in labor markets?,"[""Energy vs. accuracy"", ""Latency vs. cost"", ""Open-source vs. proprietary models""]","[""Information assurance vs. AI intelligence (learning capacity/predictive accuracy)""]","Lei Chen et al., When Assurance Undermines Intelligence: The Efficiency Costs of Data Governance in AI-Enabled Labor Markets (arXiv:2511.01923)",“creating a fundamental tension between information assurance … and artificial intelligence—the learning capacity and predictive accuracy of models.”,Strict data governance policies can sometimes reduce the flexibility of algorithmic systems that rely on diverse datasets.,
,24,How does prompt English proficiency affect code correctness in LLM-assisted software tasks?,"[""No effect across models"", ""Lower proficiency yields more correct code"", ""Only affects runtime, not correctness""]","[""Higher-proficiency prompts yielded more correct code across all models""]","Ruksit Rojpaisarnkit et al., How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks (arXiv:2511.04115)",“we found that higher-proficiency prompts consistently yielded more correct code across all models.”,"Programming assistance tools often rely on precise language, and linguistic clarity can strongly influence generated outputs.",
,25,"In design selection using GenAI concepts, which representation unexpectedly led to the best ability to select optimal designs?","[""Visual renderings only"", ""Combined visual+numerical"", ""Text-only descriptions""]","[""Only numerical design performance data""]","Zeda Xu et al., Ceci N’est Pas un Drone: Investigating the Impact of Design Representation on Decision Making When Using GenAI (arXiv:2511.03131)","“Unexpectedly, we found that providing only numerical design performance data can lead to the best ability to select optimal designs.”",Decision-making studies often show that simplified quantitative displays reduce cognitive bias compared to rich visuals.,
,26,What risk can standard visualizations introduce for MLLMs in network-bridge detection tasks?,"[""Always reduce confidence"", ""Guarantee correctness"", ""Eliminate hallucinations""]","[""Create a strong bias toward accepting/refuting a bridge regardless of truth""]","Timo Brand et al., Visualization Biases MLLM's Decision Making in Network Data Tasks (arXiv:2511.03617)",“standard visualization techniques create a strong bias towards accepting or refuting the presence of a bridge — independently of whether or not a bridge actually exists…”,Human perception and machine interpretation can both be influenced by visual framing and color-coded cues.,
,27,What does G-TRACE quantify in the context of GenAI?,"[""Only training FLOPs"", ""Only GPU memory"", ""Only storage footprint""]","[""Training- and inference-related emissions across modalities and geographies""]","Zahida Kausar et al., Quantifying the Climate Risk of Generative AI: Region-Aware Carbon Accounting with G-TRACE (arXiv:2511.04776)",“quantifies training- and inference-related emissions across modalities and deployment geographies.”,Energy consumption from large-scale AI deployments increasingly drives interest in sustainable model design.,
,28,What is the GENIUS project’s aim regarding GenAI in software engineering?,"[""Replace code reviews with GenAI entirely"", ""Focus only on bug-fixing"", ""Ignore privacy and security""]","[""Advance AI integration across all SDLC phases, aligning technical innovation with business relevance""]","Robin Gröpler et al., The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the GENIUS Project (arXiv:2511.01348)",“aims to address these challenges by advancing AI integration across all SDLC phases… aligning technical innovation with business relevance.”,"Modern software life cycles increasingly combine automated generation, human oversight, and iterative quality evaluation.",
,29,What is the paper’s position on using GenAI for qualitative coding methodologies?,"[""Strongly recommended"", ""Equivalent to human coding"", ""Applicable without documentation""]","[""Not methodologically valid; risks undermining robustness/trustworthiness""]","Maria Couto Teixeira et al., Generative Artificial Intelligence in Qualitative Research Methods: Between Hype and Risks? (arXiv:2511.08461)","“we propose that genAI is not methodologically valid within qualitative inquiries, and its use risks undermining the robustness and trustworthiness…”","Qualitative research relies on researcher interpretation, which automated text models may struggle to reproduce accurately.",
,30,"What market structure can lead to a lose-lose outcome in GenAI content procurement, per the paper?","[""Two-layer platform-creator"", ""Monopoly creator market"", ""Perfect competition with no contracts""]","[""Three-layer market including data intermediaries with large pre-signed contracts""]","Rui Ai et al., GenAI vs. Human Creators: Procurement Mechanism Design in Two-/Three-Layer Markets (arXiv:2511.06559)",“this three-layer market can result in a lose-lose outcome… as large pre-signed contracts distort creators’ incentives…”,"Intermediary platforms often reshape incentives between suppliers and end users, sometimes reducing overall efficiency.",
,31,Which bioinformatics areas are highlighted as benefiting from GenAI?,"[""Only time-series forecasting"", ""Only robotics control"", ""Only web search""]","[""Genomics, proteomics, transcriptomics, structural biology, and drug discovery""]","Riasad Alvi et al., Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances (arXiv:2511.03354)","“advancements in genomics, proteomics, transcriptomics, structural biology, and drug discovery.”","Bioinformatics increasingly uses AI to analyze molecular sequences, structures, and therapeutic design spaces.",
,32,What is the core idea behind the proposed scalable meta-learning of interpretable models?,"[""Train only on optimal trees"", ""Abandon interpretability"", ""Use purely unsupervised clustering""]","[""Generate synthetic near-optimal decision trees for pre-training (MetaTree)""]","Kyaw Hpone Myint et al., Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations (arXiv:2511.04000)",“samples near-optimal decision trees synthetically… Using the MetaTree transformer architecture…”,Synthetic data generation helps meta-learning systems generalize to diverse tasks without costly manual labeling.,
,33,Why do the authors argue CS1 should assess problem decomposition in the GenAI era?,"[""Because students write less code"", ""Because GenAI reduces cheating"", ""Because prompts replace design""]","[""Students can generate large quantities of code; decomposition remains critical to design""]","Samvrit Srinath et al., Assessing Problem Decomposition in CS1 for the GenAI Era (arXiv:2511.05764)","“With the rise of generative AI (GenAI), students… are able to generate large quantities of code… important to equip them with the ability to decompose problems.”",Problem decomposition is a fundamental skill that enables systematic reasoning regardless of coding automation tools.,
,34,What problem does TalkSketch aim to mitigate in idea generation?,"[""Inability to draw"", ""GPU memory limits"", ""Network latency""]","[""Text-only prompting disrupts creative flow; integrates sketch + speech for fluid ideation""]","Weiyan Shi et al., TalkSketch: Multimodal Generative AI for Real-time Sketch Ideation with Speech (arXiv:2511.05817)",“text-based prompting disrupts creative flow… we developed TalkSketch… integrates freehand drawing with real-time speech input.”,Creativity research suggests that multimodal interaction helps users maintain focus and fluency during ideation.,
,35,What bias pattern did the educational LLM benchmark reveal for gender counterfactuals?,"[""No differences across models"", ""Only explicit cues matter"", ""Bias reversed (female-to-male larger)""]","[""Implicit manipulations induced larger semantic shifts for male-female than female-male""]","Yishan Du et al., Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback (arXiv:2511.08225)",“implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male.”,Gender bias detection often relies on controlled language perturbations that test subtle semantic asymmetries.,
,36,Which HMI aspect does the automotive GenAI review emphasize via a case study?,"[""Only perception datasets"", ""Only manufacturing robots"", ""Only battery management""]","[""Voice-based HMI (e.g., MBUX Virtual Assistant) for more natural, proactive, personalized interactions""]","Chaitanya Shinde et al., Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case Study on In-Vehicle Experience (arXiv:2511.00026)","“a case study on Mercedes Benz’s MBUX Virtual Assistant… more natural, proactive, and personalized in-car interactions…”",Automotive HMI design increasingly integrates conversational agents to enhance driver engagement and safety.,
,37,What does the “Deception Decoder” framework aim to help users identify?,"[""Only spam emails"", ""Only phishing websites"", ""Only model copyrights""]","[""AI-generated misinformation/disinformation across text, image, and video on social media""]","C. Bowman Kerbage et al., Deception Decoder: Proposing a Human-Focused Framework for Identifying AI-Generated Content on Social Media (arXiv:2511.05555)","“designed to support general users in identifying AI-generated misinformation and disinformation across text, image, and video.”","Online misinformation increasingly leverages multimodal AI tools, requiring public literacy frameworks for detection.",
,38,What gap did the brownfield programming study find regarding GenAI coding assistants?,"[""Worse performance and worse comprehension"", ""Better comprehension only"", ""No performance gains""]","[""Improved performance (time, tests passed) without improved comprehension — a comprehension-performance gap""]","Yunhan Qiao et al., Comprehension-Performance Gap in GenAI-Assisted Brownfield Programming: A Replication and Extension (arXiv:2511.02922)",“Copilot significantly reduced task time and increased the number of test cases passed… comprehension scores did not differ… revealing a comprehension-performance gap.”,Developers may complete coding tasks faster with AI tools but still struggle to understand complex legacy systems.,
,39,What does the proposed Meta-GFlowNet enable in mobile wireless systems?,"[""Static environment optimization only"", ""Requires labeled data"", ""Slower adaptation than retraining""]","[""Rapid adaptation to dynamic conditions via model-agnostic meta-learning without labeled data""]","Zhihao Tao et al., Meta-Learning-Driven GFlowNets for 3D Directional Modulation in Mobile Wireless Systems (arXiv:2511.06188)",“achieves rapid adaptation to dynamic conditions using model-agnostic meta-learning… requires no labeled data…”,Wireless communication networks often operate in dynamic environments that demand continual self-optimization.,
,40,What key shortcoming does the paper identify in the EU AI Act with respect to fairness?,"[""Too many precise metrics"", ""Over-specification of explainability terms"", ""Mandatory open-sourcing""]","[""Absence of quantifiable fairness metrics and ambiguity among transparency/explainability/interpretability""]","Mike Teodorescu et al., An Analysis of the New EU AI Act and A Proposed Standardization Framework for Machine Learning Fairness (arXiv:2510.01281)","“we find an absence of quantifiable fairness metrics and the ambiguity in terminology… transparency, explainability, and interpretability…”","Regulatory frameworks for AI increasingly need measurable criteria to assess fairness, bias, and transparency.",
NLP,41,What are the two core capabilities emphasized in Text-rich Image Understanding (TIU)?,"[""Only object detection"", ""Pure OCR text recognition"", ""Language translation""]","[""Perception and Understanding""]","Pei Fu et al., Multimodal Large Language Models for Text-rich Image Understanding (2025)",“The perception dimension focuses on visual recognition tasks ... the understanding dimension requires semantic reasoning for applications like DocVQA.”,Defines the foundational tasks that modern TIU MLLMs must jointly handle.,
,42,What distinguishes the post-LLM era of TIU models from earlier OCR-based methods?,"[""Dependence on handcrafted rules"", ""Separate perception and understanding models"", ""Manual feature engineering""]","[""Unified end-to-end sequence modeling with LLM-based attention mechanisms""]","Pei Fu et al., Multimodal Large Language Models for Text-rich Image Understanding (2025)",“MLLMs integrate LLMs with visual encoders to jointly process visual tokens and linguistic elements through unified attention mechanisms.”,Shows how LLMs eliminate modality-specific bias and enable zero-shot generalization.,
,43,What is the main goal of AMoPO (Adaptive Multi-objective Preference Optimization)?,"[""To train larger reward models"", ""To improve RLHF through PPO"", ""To combine multiple LLMs by ensembling""]","[""To achieve dynamic balance across multiple preference dimensions without reward/reference models""]","Qi Liu et al., AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models (2025)",“AMoPO aligns LLMs with diverse preferences without additional reward models or reference models.”,Represents a lightweight alternative to traditional RLHF alignment.,
,44,Which mechanism in AMoPO enables automatic dimension prioritization?,"[""Static weighting"", ""Manual label annotation"", ""Deterministic rule-based scoring""]","[""Adaptive weight assignment modeling the generation space as a Gaussian distribution""]","Qi Liu et al., AMoPO (2025)",“Sampling from this Gaussian distribution dynamically determines the importance weight for each dimension.”,Introduces a novel probabilistic scheme for balancing human preferences.,
,45,What problem does MERGE aim to solve in Generative Retrieval systems?,"[""Slow document ranking"", ""Duplicate indexing errors"", ""Lack of semantic relevance in DocID generation""]","[""Improving semantic relevance by learning multi-level document identifiers through query bridging""]","Fuwei Zhang et al., Multi-level Relevance Document Identifier Learning for Generative Retrieval (ACL 2025)","“We propose MERGE, a novel approach that utilizes multi-level document relevance to learn high-quality DocIDs.”",Enhances retrieval by encoding hierarchical semantic relationships between queries and documents.,
,46,Which module in MERGE captures hierarchical semantic information?,"[""Only a binary contrastive loss"", ""Document hashing network"", ""Sequence decoder""]","[""Inner-level multi-relevance learning module""]","Fuwei Zhang et al., MERGE (ACL 2025)",“An inner-level multi-level relevance learning module distinguishes documents with different relevance levels.”,Adds fine-grained control over document similarity in generative retrieval.,
,47,What does the paper ‘Understanding the Dark Side of LLMs’ Intrinsic Self-Correction’ primarily reveal?,"[""That self-correction always improves answers"", ""That prompt bias never occurs"", ""That oracle labels are unnecessary""]","[""That intrinsic self-correction can fail and introduce human-like bias and answer wavering""]","Qingjie Zhang et al., Understanding the Dark Side of LLMs’ Intrinsic Self-Correction (2025)","“Self-correction can fail in SOTA LLMs, causing answer wavering and prompt bias on simple tasks.”",Shows limitations of unsupervised self-feedback mechanisms in LLMs.,
,48,Which strategy did the authors propose to alleviate self-correction failures?,"[""Longer prompts"", ""Reinforcement learning loops"", ""Adding oracle labels""]","[""Question repeating and light supervised fine-tuning with few samples""]","Qingjie Zhang et al., Intrinsic Self-Correction (2025)",“We propose two simple yet effective methods: question repeating and SFT with <10 samples.”,Demonstrates simple post-training fixes for bias and instability.,
,49,What core problem does LEAF address in traffic flow forecasting?,"[""Low model capacity"", ""Overfitting on training data"", ""Dependence on static graphs""]","[""Inability of existing methods to adapt to test-time environmental changes""]","Yusheng Zhao et al., Embracing Large Language Models in Traffic Flow Forecasting (2025)",“Existing methods fall short in adapting to test-time environmental changes.”,Applies LLMs for dynamic decision-making in real-world transport systems.,
,50,How does LEAF utilize LLMs within its architecture?,"[""As a graph neural network component"", ""To directly generate traffic data"", ""To replace hypergraph modules""]","[""To select and rank the most likely prediction between dual branches at test time""]","Yusheng Zhao et al., LEAF (2025)",“A large language model is used to select the most likely result between graph and hypergraph branches.”,Leverages LLMs’ discriminative reasoning to improve forecast robustness.,
,51,What is the primary contribution of FinMME benchmark?,"[""A text-only financial QA dataset"", ""A financial news summary corpus"", ""A stock-price forecasting model""]","[""A comprehensive financial multimodal dataset for reasoning and evaluation across 18 domains and 6 asset classes""]","Junyu Luo et al., FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation (2025)","“FINMME encompasses >11,000 financial samples across 18 domains and 6 asset classes.”",Bridges a critical gap in evaluating MLLMs on finance-specific tasks.,
,52,What metric does FinMME introduce for unbiased model evaluation?,"[""BLEU score"", ""F1 metric"", ""Cross-entropy loss""]","[""FinScore""]","Junyu Luo et al., FinMME (2025)","“We introduce FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional assessment.”",Addresses accuracy and credibility requirements in financial AI systems.,
,53,What innovation does Native Sparse Attention (NSA) bring to long-context modeling?,"[""Larger batch sizes"", ""Static attention masking"", ""Sequential token processing""]","[""Dynamic hierarchical sparse strategy combining coarse compression and fine selection""]","Jingyang Yuan et al., Native Sparse Attention (2025)",“NSA employs a dynamic hierarchical sparse strategy combining coarse-grained compression with fine-grained selection.”,Improves efficiency for 64k-token contexts while maintaining accuracy.,
,54,What kind of speedup does NSA achieve over Full Attention models on 64k sequences?,"[""1.5×"", ""3×"", ""5×""]","[""Up to 11.6× speedup across decoding, forward and backward propagation""]","Jingyang Yuan et al., NSA (2025)",“NSA achieves 11.6× decoding speedup over Full Attention on 64k-length sequences.”,Quantifies real-world hardware-aligned performance gain.,
,55,What problem does the GCSE model aim to solve in unsupervised sentence embedding?,"[""Low model size"", ""Need for supervised labels"", ""Slow tokenization""]","[""Low data diversity and high data noise in LLM-based augmentation""]","Peichao Lai et al., Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation (2025)",“Existing methods encounter limited data diversity and high data noise.”,Improves contrastive embedding learning using knowledge graphs and Gaussian decay.,
,56,How does the Gaussian-decayed function help in GCSE?,"[""Amplifies false negatives"", ""Eliminates all hard samples"", ""Freezes model gradients""]","[""Reduces the impact of false hard negatives by down-weighting their gradients""]","Peichao Lai et al., GCSE (2025)",“GCSE uses a Gaussian-decayed function to limit the impact of false hard negative samples.”,Stabilizes contrastive training and enhances representation quality.,
,57,What is the key architectural feature of LLaMA-Omni?,"[""ASR + TTS cascaded design"", ""Text-only input pipeline"", ""Offline speech processing""]","[""End-to-end speech encoder–LLM–decoder architecture with low-latency speech interaction""]","Qingkai Fang et al., LLaMA-Omni: Seamless Speech Interaction with LLMs (ICLR 2025)","“LLaMA-Omni integrates a speech encoder, speech adaptor, LLM, and streaming speech decoder.”",First open-source architecture for real-time speech LLMs.,
,58,What is the approximate response latency achieved by LLaMA-Omni?,"[""> 1 second"", ""500 ms"", ""800 ms""]","[""As low as 236 milliseconds""]","Qingkai Fang et al., LLaMA-Omni (ICLR 2025)",“LLaMA-Omni achieves a response latency as low as 236 ms.”,Demonstrates efficiency comparable to GPT-4o for speech tasks.,
,59,What main idea underpins LLaVA-Mini’s efficiency in multimodal modeling?,"[""Increasing LLM size"", ""Adding more vision tokens"", ""Using image compression only""]","[""Reducing vision tokens to a single token through modality pre-fusion""]","Shaolei Zhang et al., LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token (ICLR 2025)",“LLaVA-Mini introduces modality pre-fusion ... compressing 576 vision tokens into one.”,Radically reduces FLOPs while preserving visual understanding.,
,60,How much FLOPs reduction does LLaVA-Mini achieve compared to LLaVA-v1.5?,"[""25 %"", ""50 %"", ""60 %""]","[""77 %""]","Shaolei Zhang et al., LLaVA-Mini (ICLR 2025)",“Efficiency analyses reveal that LLaVA-Mini reduces FLOPs by 77 % and achieves 2.92× speed.”,Quantifies its computational advantage for real-time multimodal tasks.,
Robotics,61,"In robotic manipulation, what innovation enables unsupervised grasp generation for irregular, tool-like objects?","[“Brute-force grasp enumeration”, “Heavy energy-function tuning”, “Assuming regular object geometry only”]","[“Decoupling complex geometric computation via a Contact Field data-structure”, “High-performance procedural grasp synthesis”]","Yin & Abbeel, Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields (arXiv:2511.07418)","“decoupling … via a simple, efficient data-structure — the Contact Field … unsupervised grasp generation for irregular, tool-like objects.”",Robotic grasping often struggles when objects have unusual shapes or tools with non-standard geometry.,
,62,What approach allows robotic manipulation models to align input and output modalities for efficient training?,"[“Full precision large models only”, “Separate language and vision branches with no alignment”, “Ignoring output modality altogether”]","[“Input-Output alignment for efficient 3D manipulation learning with vision-language models”, “BitVLA / BridgeVLA framework”]","Li et ., BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models (arXiv:2506.07961)",“BridgeVLA … input-output alignment … efficient 3D manipulation learning with vision-language models.”,"In manipulation tasks, robots receive multi-modal input (vision, text) and must output action trajectories — alignment helps efficiency.",
,63,"In multi-robot systems, what methodology supports very-large-scale task allocation in challenging environments?","[“Single robot planning only”, “Static task assignment with no redistribution”, “Homogeneous robots only”]","[“Robot redistribution strategy for very large-scale multi-robot task allocation”, “Heterogeneous teams in challenging environments”]","Lee, Sim & Nam, Very Large-scale Multi-Robot Task Allocation in Challenging Environments via Robot Redistribution (arXiv:2506.07293)",“Very large-scale multi-robot task allocation … via robot redistribution … heterogeneous teams.”,"As robot fleets grow, distribution strategies and heterogeneity become key for complex environments (e.g., disaster zones).",
,64,Which technique enables efficient visuomotor policy learning using frequency consistency constraints?,"[“Standard flow-matching only”, “Ignoring consistency across frequencies”, “Single-objective RL only”]","[“Flow-based visuomotor policy via frequency-consistency (FreqPolicy)”, “Efficient policy learning in robotics”]","Su et ., FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency (arXiv:2506.08822)",“Efficient flow-based visuomotor policy … frequency-consistency … enables efficient policy learning in robotics.”,Visuomotor policies map visual inputs to motor actions; enforcing consistency across temporal/spatial frequencies can stabilize learning.,
,65,What design enables a robot to learn lifelike whole-body locomotion and manipulation through “skill blending”?,"[“Single fixed gait only”, “Separate locomotion/manipulation modules with no blending”, “Hard-coded sequences only”]","[“SkillBlender: blending multiple skills for versatile humanoid whole-body loco-manipulation”, “Humanoid versatile learning”]","Kuang et ., SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending (arXiv:2506.09366)",“SkillBlender … versatile humanoid whole-body loco-manipulation via skill blending.”,"For humanoids or complex legged robots, combining manipulation with locomotion enables more natural behaviours (e.g., walking and picking up something).",
,66,What method supports generating instance-level 3D scene graphs using LiDAR-camera fusion for robots?,"[“Only 2D scene graphs”, “Only LiDAR with no camera fusion”, “No instance-level segmentation”]","[“Instance-Level 3D Scene Graphs via Room Prior Guided LiDAR-Camera Fusion (IRS)”, “Robust mapping for robotics”]","Chen et ., IRS: Instance-Level 3D Scene Graphs via Room Prior Guided LiDAR-Camera Fusion (arXiv:2506.06804)",“Instance-Level 3D Scene Graphs … guided LiDAR-camera fusion … mapping for robotics.”,"Robots operating in complex spaces (indoor/outdoor) must build rich scene understanding beyond mere occupancy grids (e.g., objects, relationships).",
,67,What approach enables zero-shot object navigation using a 3D voxel-based belief map?,"[“2D belief map only”, “Pre-trained on target objects only”, “No voxel representation”]","[“BeliefMapNav: 3D voxel-based belief map for zero-shot object navigation”, “Generalizable object navigation”]","Zhou et ., BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation (arXiv:2506.06487)",“BeliefMapNav … 3D voxel-based belief map … zero-shot object navigation.”,Navigation in unknown environments often requires belief about unseen objects/areas; a voxel belief map helps generalize to new targets.,
,68,What technique aids real-time adaptation to dynamic human intentions during collaboration with robots?,"[“Static intention prediction only”, “Only long-horizon planning with no switching”, “No collaboration modelling”]","[“Hierarchical intention tracking with switching trees for real-time adaptation”, “Human-robot collaborative adaptation”]","Huang et ., Hierarchical Intention Tracking with Switching Trees for Real-Time Adaptation to Dynamic Human Intentions during Collaboration (arXiv:2506.07004)",“Hierarchical intention tracking … switching trees … real-time adaptation to dynamic human intentions.”,"In human-robot collaboration settings (factories, assistive robots), the robot must rapidly adjust to changing human intent in real time.",
,69,What innovation supports end-to-end multimodal planning for autonomous driving via trajectory scoring?,"[“Single modality only (vision)”, “Hand-coded heuristics only”, “No multimodal fusion”]","[“Generalized trajectory scoring for end-to-end multimodal planning in driving”, “High-performing multimodal planning”]","Li et ., Generalized Trajectory Scoring for End-to-End Multimodal Planning (arXiv:2506.06664)",“Generalized trajectory scoring … end-to-end multimodal planning … driving domain.”,"Autonomous driving systems increasingly rely on combining multiple modalities (e.g., LiDAR, vision, radar) to plan trajectories.",
,70,"What platform design brings agility, squeezability and collision resilience together in quadrotor robotics?","[“Rigid only design”, “No collision resilience”, “Only high agility at expense of safety”]","[“Bioinspired soft quadrotors with agility, squeezability & collision resilience”, “Soft-robotics UAV innovation”]","(See recent list: arXiv:2511.0xxx – e.g., “Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and Collision Resilience”)","“Bioinspired soft quadrotors … agility, squeezability, collision resilience … novel UAV robotics.”",Unmanned aerial vehicles (UAVs) operating in cluttered or indoors spaces benefit from soft or deformable designs that handle collisions gracefully.,
,71,What enables a robot to form terrain-aware task-driven 3D scene graphs in outdoor environments?,"[“Indoor only scene graph models”, “No terrain awareness”, “2D graphs only”]","[“Terrain-aware task-driven 3D scene graph generation in outdoor environments”, “Improved understanding of scenes for tasks”]","Samuelson et ., Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor Environments (arXiv:2506.06562)",“Terrain-aware task-driven 3D scene graph generation … outdoor environments.”,"Outdoor robotics (e.g., agriculture, search & rescue) requires modelling terrain features and how tasks map onto them — richer than indoor flat terrain.",
,72,Which approach enhances situational awareness in underwater robotics via multimodal spatial perception?,"[“Only sonar sensing”, “No spatial fusion”, “Single-mode perception only”]","[“Enhancing situational awareness in underwater robotics with multimodal spatial perception”, “Multi-modal fusion for underwater robots”]","Kaveti et ., Enhancing Situational Awareness in Underwater Robotics with Multi-modal Spatial Perception (arXiv:2506.06476)",“Multi-modal spatial perception … underwater robotics … enhanced situational awareness.”,"Underwater robots must operate under limited visibility and sensor constraints, making multimodal fusion (e.g., sonar + vision) very valuable.",
,73,What low-cost open-source robot platform enables large-scale swarm robotics experiments?,"[“High-cost proprietary robot”, “Single robot type only”, “No communication hardware”]","[“Low-cost open-hardware robot platform for swarm robotics”, “Enables large-scale many-agent experiments”]","San-Miguel-Tello et ., Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications (arXiv:2506.09494)",“Affordable hardware platforms … human-demonstration acquisition … large-scale robotics.”,"Swarm robotics research benefits from affordable, exchangeable hardware so many robots can collaborate and researchers can scale experiments.",
,74,What method uses human-assisted policy refinement through action preference optimization in robotic learning?,"[“Pure RL with no human input”, “Only imitation learning with no preference”, “No policy refinement”]","[“Human-assisted robotic policy refinement via action-preference optimization”, “Interactive learning improvement”]","Xia et ., Human-assisted Robotic Policy Refinement via Action Preference Optimization (arXiv:2506.07127)",“Human-assisted robotic policy refinement … action preference optimization.”,Bringing humans into the loop for refining robot behaviours (via preferences or feedback) helps align robot policies with human expectations.,
,75,What technique improves sim-to-real adaptation for point-cloud segmentation in industrial human-robot collaboration?,"[“Only visual input adaptation”, “No domain adaptation at all”, “Single-sensor modality only”]","[“Sim2Real domain adaptation algorithm for point-cloud segmentation in industrial human-robot collaboration”, “Bridging simulation→real world”]","Mohammadi Amin et ., Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments (arXiv:2506.09552)",“Sim2Real domain adaptation … point-cloud segmentation … human-robot collaboration.”,"Industrial robots often train in simulation (safety, cost) but must perform reliably in the real world — bridging that gap is critical.",
,76,What planner enforces multiple inequality constraints using learned signed distance fields in motion planning?,"[“Standard MPPI with no constraints”, “Only static obstacle avoidance”, “No learned fields”]","[“Barrier-Rate-guided MPPI (BR-MPPI) enforcing multiple inequality constraints with learned SDF”, “Advanced motion planning”]","Parwana et ., BR-MPPI: Barrier Rate guided MPPI for Enforcing Multiple Inequality Constraints with Learned Signed Distance Field (arXiv:2506.07325)",“Barrier rate guided MPPI … multiple inequality constraints … learned signed distance field.”,"Robots must operate under many constraints (joint limits, collisions, workspace bounds); planners that incorporate these explicitly are more robust.",
,77,What framework uses a neuro-symbolic approach for robot learning by incorporating Bayesian inverse physics?,"[“Pure symbolic reasoning no learning”, “Pure neural network with no physics”, “No inverse physics modelling”]","[“Bayesian inverse physics for neuro-symbolic robot learning”, “Combining physics-based modelling and learning”]","Arriaga et ., Bayesian Inverse Physics for Neuro-Symbolic Robot Learning (arXiv:2506.08756)",“Neuro-symbolic robot learning … Bayesian inverse physics … combining physics & learning.”,"For robots to generalize well, combining geometry/physics knowledge with learning (neuro-symbolic) can reduce sample complexity and improve performance.",
,78,What innovation enables autonomous prosthetic-hand control without biosignals using imitation learning?,"[“Only using EMG biosignals”, “No imitation learning”, “Rigid prosthetic with no autonomy”]","[“Biosignals-free autonomous prosthetic hand control via imitation learning”, “Assistive robotics advancement”]","Shi et ., Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning (arXiv:2506.08795)",“Biosignals-free autonomous prosthetic hand control … imitation learning.”,Prosthetic hands traditionally rely on biosignals (EMG); removing that reliance opens accessibility for more users and settings.,
,79,What method optimizes a segmented varying-curved foot design for bipedal robot walking?,"[“Standard fixed foot design only”, “No optimization of foot shape”, “Only rolling foot design”]","[“Ellipse-based segmented varying-curved foot design for biped robot walking”, “Foot-design optimisation for locomotion”]","Chen et ., Model Analysis and Design of Ellipse-Based Segmented Varying Curved Foot for Biped Robot Walking (arXiv:2506.07283)",“Model analysis and design … ellipse-based segmented varying curved foot … biped robot walking.”,"Foot design (shape, segmentation) can significantly affect bipedal robot stability, energy use, and terrain adaptability.",
,80,What method achieves source-seeking for a wheeled robot using only differential wheeled experiments and no model of the field?,"[“Full field model required”, “Standard PID only”, “No wireless/field sensing used”]","[“Model-Free real-time unicycle-based source-seeking with differential-wheeled experiments”, “Reactive navigation without explicit field model”]","Elgohary et ., Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments (arXiv:2501.02184)",“Model-free and real-time … unicycle-based source-seeking … differential-wheeled robot experiments.”,"In some exploration or environmental monitoring tasks, robots must seek sources (e.g., chemical leak) without a pre-existing model of the field — model-free real-time methods help.",