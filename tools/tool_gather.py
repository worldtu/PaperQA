"""
PaperQA â€” Gather Evidence Tool
Implements: MMR retrieval + Summary LLM + LLM-based scoring (1â€“10 relevance)
"""
import os
import warnings
os.environ["TOKENIZERS_PARALLELISM"] = "false"
warnings.filterwarnings("ignore", message=".*tokenizers.*")

from concurrent.futures import ThreadPoolExecutor
import re
import json
import random
import asyncio
from typing import List, Optional
import numpy as np
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
load_dotenv()
from paperqa.schemas import Chunk, Evidence
from paperqa.settings import (
    EVIDENCE_TOPK_CONTEXT,
    MMR_LAMBDA,
    MMR_INITIAL_TOPK,
)
# ----------------------------------------------------
# Retriever: use MMR to select diverse & relevant chunks
# ----------------------------------------------------
class Retriever:
    def __init__(self, chunks: List[Chunk], lambda_param: float = MMR_LAMBDA, initial_topk: int = MMR_INITIAL_TOPK):
        self.chunks = chunks
        self.lambda_param = lambda_param
        self.initial_topk = initial_topk  # å…ˆåšç²—æ’ï¼Œå¬å› top-N ä¸ªæœ€ç›¸å…³çš„
        
    def mmr_search(self, query_emb: np.ndarray, k: int = EVIDENCE_TOPK_CONTEXT) -> List[Chunk]:
        """
        Return top-k diverse and relevant chunks using Maximal Marginal Relevance (MMR).
        
        æ”¹è¿›ï¼šä¸¤é˜¶æ®µæ£€ç´¢
        1. ç²—æ’ï¼šå…ˆç”¨ç›¸ä¼¼åº¦å¬å› top-N ä¸ªæœ€ç›¸å…³çš„ chunks
        2. ç²¾æ’ï¼šåœ¨ top-N ä¸Šç”¨ MMR é€‰æ‹©æ—¢ç›¸å…³åˆå¤šæ ·åŒ–çš„ k ä¸ª chunks
        """
        # Check that all chunks have embeddings (should be set in search.ingest())
        chunks_without_embedding = [c for c in self.chunks if c.embedding is None]
        if chunks_without_embedding:
            raise ValueError(f"{len(chunks_without_embedding)} chunks missing embeddings. "
                           "Embeddings should be generated in SearchTool.ingest().")
        
        # Flatten query_emb if it's 2D
        if len(query_emb.shape) > 1:
            query_emb = query_emb.flatten()
        
        # è®¡ç®—æ‰€æœ‰ chunks ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
        docs = np.stack([np.array(c.embedding) for c in self.chunks])
        sim_to_query = np.dot(docs, query_emb) / (
            np.linalg.norm(docs, axis=1) * np.linalg.norm(query_emb) + 1e-8  # é˜²æ­¢é™¤é›¶
        )
        
        # ğŸ” é˜¶æ®µ1ï¼šç²—æ’ - å…ˆå¬å› top-N ä¸ªæœ€ç›¸å…³çš„ chunks
        # ç¡®ä¿ initial_topk ä¸è¶…è¿‡æ€» chunks æ•°
        actual_topk = min(self.initial_topk, len(self.chunks))
        top_indices = np.argsort(sim_to_query)[::-1][:actual_topk]  # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        
        # åªåœ¨ top-N å€™é€‰é›†ä¸Šè®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰
        candidate_docs = docs[top_indices]
        candidate_sim_to_query = sim_to_query[top_indices]
        candidate_sim_matrix = np.dot(candidate_docs, candidate_docs.T)
        
        # ğŸ” é˜¶æ®µ2ï¼šç²¾æ’ - åœ¨ top-N ä¸Šç”¨ MMR é€‰æ‹© k ä¸ª
        selected, remaining = [], list(range(len(top_indices)))

        while len(selected) < k and remaining:
            mmr_scores = []
            for i in remaining:
                # ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
                relevance = candidate_sim_to_query[i]
                # ä¸å·²é€‰æ–‡æ¡£çš„æœ€å¤§å†—ä½™åº¦
                redundancy = max([candidate_sim_matrix[i, j] for j in selected], default=0)
                # MMR åˆ†æ•°ï¼šå¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
                score = self.lambda_param * relevance - (1 - self.lambda_param) * redundancy
                mmr_scores.append(score)
            
            if not mmr_scores:
                break
                
            next_idx = remaining[np.argmax(mmr_scores)]
            selected.append(next_idx)
            remaining.remove(next_idx)

        # è¿”å›åŸå§‹ chunksï¼ˆé€šè¿‡ top_indices æ˜ å°„å›å»ï¼‰
        return [self.chunks[top_indices[i]] for i in selected]

# ----------------------------------------------------
# Summarizer: LLM summarization + LLM relevance scoring
# ----------------------------------------------------
class Summarizer:
    def __init__(self, summary_model: str = "meta-llama/Llama-3.1-8B-Instruct:novita", score_model: str = "meta-llama/Llama-3.1-8B-Instruct:novita"):
        # ä½¿ç”¨ Hugging Face è·¯ç”±å™¨ API
        self.client = OpenAI(
            base_url="https://router.huggingface.co/v1",
            api_key=os.environ.get("HF_TOKEN", ""),
        )
        self.score_model = score_model      # Llama 3.1 8B for scoring

    def summarize_and_score(self, chunk: Chunk, question: str) -> Evidence:
        """
        Use unified prompt to get both summary and score in one response.
        Uses Llama 3.1 8B for combined summary and scoring.
        """
        prompt = f"""Summarize the text below to help answer a question. Do not directly answer the question, instead summarize to give evidence to help answer the question. Reply 'Not applicable' if
text is irrelevant. Use concise summary length. At the end of your response, provide a score from 1-10 on a newline indicating relevance to question. Do not explain your score.

Excerpt from citation:
{chunk.text}

Question: {question}

Relevant Information Summary:
"""
        response = self.client.chat.completions.create(
            model=self.score_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        content = response.choices[0].message.content.strip()
        
        # Parse summary and score from response
        if "\n" in content:
            *summary_lines, last_line = content.split("\n")
            summary = "\n".join(summary_lines).strip()
            # Extract score from last line
            match = re.search(r"(\d+(\.\d+)?)", last_line)
            score = float(match.group(1)) if match else None
        else:
            summary = content
            score = None

        return Evidence(chunk_id=chunk.chunk_id, summary=summary, score=score)


# ----------------------------------------------------
# GatherTool: orchestrates retrieval â†’ summarize â†’ score
# ----------------------------------------------------
class GatherTool:
    def __init__(self, retriever: Retriever, summarizer: Summarizer, embedder: Optional[SentenceTransformer] = None):
        self.retriever = retriever
        self.summarizer = summarizer
        self.embedder = embedder
        if self.embedder is None:
            self.embedder = SentenceTransformer("BAAI/bge-base-en")

    def gather(self, question: str, query_emb: Optional[np.ndarray] = None) -> List[Evidence]:
        """
        1ï¸âƒ£ Retrieve diverse chunks (MMR)
        2ï¸âƒ£ Summarize each chunk with LLM (1â€“10 scoring)
        3ï¸âƒ£ Sort by score
        Args:
            question: The question to answer
            query_emb: Optional pre-computed query embedding. If None, will generate from question.
        """
        # Generate query embedding if not provided
        if query_emb is None:
            query_emb = self.embedder.encode([question], convert_to_numpy=True, normalize_embeddings=True)[0]
        
        # Step 1 â€” MMR retrieval
        chunks: List[Chunk] = self.retriever.mmr_search(query_emb, k=EVIDENCE_TOPK_CONTEXT)
 
        # Step 2 â€” Concurrent summarization + scoring
        evidences: List[Evidence] = self._batch_summarize(chunks, question)

        # Step 3 â€” Sort by score (filter out None scores first)
        evidences = [e for e in evidences if e.score is not None]
        evidences.sort(key=lambda e: e.score, reverse=True)
        return evidences

    def _batch_summarize(self, chunks: List[Chunk], question: str) -> List[Evidence]:
        """Run summarization + scoring concurrently."""
        from contextlib import redirect_stderr
        from io import StringIO
        
        evidences = []
        # ä¸´æ—¶é‡å®šå‘ stderr ä»¥æŠ‘åˆ¶ tokenizers è­¦å‘Š
        stderr_buffer = StringIO()
        with redirect_stderr(stderr_buffer):
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(self.summarizer.summarize_and_score, c, question) for c in chunks]
                for f in futures:
                    evidences.append(f.result())
        return evidences


if __name__ == "__main__":
    from paperqa.tools.tool_search import SearchTool
    
    async def test_gather_evidence():
        try:
            litqa_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "litqa-v0.jsonl")
            with open(litqa_path, "r") as f:
                samples = [json.loads(line) for line in f if line.strip()]
            # è¿‡æ»¤æ‰æ²¡æœ‰ question å­—æ®µçš„è¡Œï¼ˆå¦‚ canary è¡Œï¼‰
            samples = [s for s in samples if "question" in s]
            if not samples:
                print("âŒ No valid questions found in litqa-v0.jsonl")
                return
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªé—®é¢˜
            random.seed(123)  # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºéšæœºç§å­
            sample = random.choice(samples)
            question = sample["question"]
            
            print(f"\nğŸ§ª Testing LitQA Question:\n{question}\n")
            if "ideal" in sample:
                print(f"ğŸ“ Ideal Answer: {sample['ideal']}\n")
            
            search_tool = SearchTool(None, None, None, None, None, None, None)
            hits = await search_tool.smart_search(question, min_hits=3, max_rounds=2)
            print(f"ğŸ“š Found {len(hits)} papers")
            
            chunks = await search_tool.ingest(hits)
            print(f"ğŸ“„ Generated {len(chunks)} chunks\n")
            
            if not chunks:
                print("âš ï¸ No chunks available")
                return
            
            retriever = Retriever(chunks)
            summarizer = Summarizer(summary_model="meta-llama/Llama-3.1-8B-Instruct:novita", score_model="meta-llama/Llama-3.1-8B-Instruct:novita")
            gather_tool = GatherTool(retriever, summarizer, embedder=search_tool.embedder)
            
            print("ğŸ” Gathering evidence...\n")
            evidences = gather_tool.gather(question)
            
            print(f"âœ… Collected {len(evidences)} evidences:\n")
            # åˆ›å»º chunk_id åˆ° chunk çš„æ˜ å°„ï¼Œæ–¹ä¾¿æŸ¥çœ‹åŸæ–‡
            chunk_dict = {chunk.chunk_id: chunk for chunk in chunks}
            
            for i, ev in enumerate(evidences[:10], 1):
                print(f"[{i}] Score: {ev.score}/10")
                print(f"    Chunk ID: {ev.chunk_id[:60]}...")
                
                # æ˜¾ç¤ºåŸå§‹ chunk æ–‡æœ¬
                if ev.chunk_id in chunk_dict:
                    original_text = chunk_dict[ev.chunk_id].text[:-1].replace('\n', ' ')
                    print(f"    Original Text: {original_text}...")
            
                print(f"    Summary: {ev.summary[:-1]}...")
                
                print()
                
        except Exception as e:
            print(f"âŒ Error: {e}")
            import traceback
            traceback.print_exc()

    asyncio.run(test_gather_evidence())
